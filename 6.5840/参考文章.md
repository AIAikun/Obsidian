https://thesquareplanet.com/blog/students-guide-to-raft/#applying-client-operations
**学生 Raft 指南（阅读时间约 30 分钟）**
**背景**
在我们深入探讨 Raft 之前，了解一些背景信息可能有所帮助。6.824 过去有一套基于 Paxos 的、用 Go 语言构建的实验；选择 Go 既因为它对学生来说易于学习，也因为它非常适合编写并发、分布式的应用程序（goroutine 尤其方便）。在四个实验的过程中，学生需要构建一个容错、分片的键值存储系统。第一个实验让他们构建一个基于共识的日志库，第二个实验在其之上添加一个键值存储，第三个实验将键空间分片到多个容错集群中，并由一个容错的分片主节点处理配置变更。我们还有第四个实验，学生必须处理机器的故障和恢复，无论其磁盘是否完好。这个实验是学生可选的期末项目。

今年，我们决定使用 Raft 重写所有这些实验。前三个实验内容基本不变，但第四个实验被取消了，因为持久化和故障恢复已内置在 Raft 中。本文将主要讨论我们在第一个实验中的经验，因为它与 Raft 最直接相关，不过我也会涉及在 Raft 之上构建应用程序（如在第二个实验中）。

Raft，对于刚刚了解它的读者，最好用该协议官网上的文字来描述：

> Raft 是一种旨在易于理解的共识算法。它在容错性和性能上与 Paxos 等效。不同之处在于，它被分解为相对独立的子问题，并且清晰地解决了实际系统所需的所有主要部分。我们希望 Raft 能使共识算法被更广泛的受众所接受，并且这些更广泛的受众能够开发出比当今现有系统更高质量的各种基于共识的系统。

像[这个](https://raft.github.io/)可视化演示能很好地概述该协议的主要组成部分，而论文则很好地解释了为什么需要这些不同的部分。如果你还没有阅读过 Raft 扩展论文，你应该在继续阅读本文之前先去读一下，因为我会假设读者对 Raft 有相当的熟悉度。

与所有分布式共识协议一样，**细节决定成败**。在没有故障的稳定状态下，Raft 的行为很容易理解，并且可以用直观的方式解释。例如，从可视化中很容易看出，假设没有故障，最终会选举出一个领导者，并且所有发送给领导者的操作最终都会被跟随者以正确的顺序应用。然而，当引入延迟消息、网络分区和故障服务器时，每一个 **如果**、**但是** 和 **并且** 都变得至关重要。特别是，我们会反复看到许多错误，仅仅是因为阅读论文时的误解或疏忽。这个问题并非 Raft 独有，在所有提供正确性保证的复杂分布式系统中都会出现。
![[Pasted image 20251023214429.png]]
**实现 Raft**
Raft 的终极指南是其论文中的**图 2**。该图规定了 Raft 服务器之间交换的每个 RPC 的行为，给出了服务器必须维护的各种不变量，并规定了特定动作应在何时发生。在本文的其余部分，我们将大量讨论图 2。**必须严格遵守它**。

图 2 定义了每个服务器在**每种**状态下，对于**每个**传入的 RPC 应该做什么，以及某些其他事情应在何时发生（例如，何时可以安全地应用日志中的条目）。起初，你可能会想把图 2 当作一种非正式的指南；读一遍，然后开始编写一个大致遵循其指示的实现。这样做，你将能快速启动并运行一个基本可用的 Raft 实现。然后问题就开始了。

事实上，图 2 **极其精确**，它所做的每一个陈述，在规范术语上都应被视为 **必须**，而不是 **应该**。例如，你可能会合理地认为，每当收到 `AppendEntries` 或 `RequestVote` RPC 时，都应该重置对等节点的选举定时器，因为这两者都表明其他对等节点要么认为自己是领导者，要么正试图成为领导者。直观上，这意味着我们不应该干扰。然而，如果你仔细阅读图 2，它说：

> 如果选举超时过去，仍未从**当前领导者**收到 `AppendEntries` RPC **或** 未**授予投票**给候选者：转换为候选者。

事实证明，这种区别**非常重要**，因为前一种实现在某些情况下可能导致**活性显著降低**。

**细节的重要性**
为了使讨论更具体，我们考虑一个让许多 6.824 学生栽跟头的例子。Raft 论文在多个地方提到了**心跳 RPC**。具体来说，领导者会偶尔（至少每个心跳间隔一次）向所有对等节点发送 `AppendEntries` RPC 以防止它们开始新的选举。如果领导者没有新的条目要发送给某个特定的对等节点，则 `AppendEntries` RPC 不包含任何条目，并被视为心跳。

我们的许多学生认为心跳在某种程度上是“特殊的”；认为当对等节点收到心跳时，应该以不同于非心跳 `AppendEntries` RPC 的方式处理它。特别是，许多人会在收到心跳时简单地重置其选举定时器，然后返回成功，**而不执行图 2 中指定的任何检查**。这是**极其危险**的。通过接受该 RPC，跟随者是在**隐式地**告诉领导者，它的日志与领导者的日志匹配，直到并包括 `AppendEntries` 参数中包含的 `prevLogIndex`。收到回复后，领导者可能会（错误地）断定某个条目已被复制到大多数服务器，并开始提交它。

许多人遇到的另一个问题（通常是在修复上述问题后立即出现）是，在收到心跳时，他们会**截断**跟随者日志中 `prevLogIndex` 之后的部分，然后附加 `AppendEntries` 参数中包含的任何条目。**这也是不正确的**。我们可以再次查看图 2：

> **如果**现有条目与新条目冲突（索引相同但任期不同），**则**删除现有条目及其后的所有条目。

这里的 **如果** 至关重要。**如果跟随者拥有领导者发送的所有条目，跟随者绝不能截断其日志。领导者发送的条目之后的任何元素都必须保留。** 这是因为我们可能从领导者那里收到**过时**的 `AppendEntries` RPC，而截断日志将意味着“收回”我们可能已经告诉领导者我们日志中已有的条目。

**调试 Raft**
不可避免地，你的 Raft 实现的第一个版本会有错误。第二个、第三个、第四个也是。总的来说，每个版本都会比前一个错误更少，并且根据经验，你的大部分错误都是由于**没有忠实地遵循图 2** 造成的。

在调试 Raft 时，通常有四个主要的错误来源：**活锁**、**不正确或不完整的 RPC 处理程序**、**未能遵循规则** 以及 **任期混淆**。死锁也是一个常见问题，但通常可以通过记录所有加锁和解锁操作，并找出你获取了但未释放的锁来调试。让我们依次考虑这些情况：

**活锁**
当你的系统发生活锁时，系统中的每个节点都在做某事，但总体而言，你的节点处于无法取得进展的状态。这在 Raft 中相当容易发生，特别是如果你没有严格遵循图 2。一种活锁场景尤其常见：没有领导者被选举出来，或者一旦领导者当选，其他某个节点就开始选举，迫使刚当选的领导者立即退位。

出现这种情况的原因有很多，但我们看到许多学生反复犯一些错误：

*   **确保你只在图 2 规定的时间点重置选举定时器。** 具体来说，你只应在以下情况下重启选举定时器：a) 你从**当前领导者**收到 `AppendEntries` RPC（即，如果 `AppendEntries` 参数中的任期已过时，你不应重置定时器）；b) 你**开始**一次选举；或 c) 你**授予**投票给另一个对等节点。
    *   最后一种情况在不可靠网络中尤为重要，因为那里的跟随者很可能拥有不同的日志；在这些情况下，你通常最终只有少数服务器能获得大多数服务器的投票。如果你在每次有人要求你投票时都重置选举定时器，那么日志过时的服务器与日志更长的服务器站出来竞选的可能性是相同的。
    *   实际上，因为拥有足够更新日志的服务器非常少，这些服务器很难在足够“平静”的环境中完成选举并当选。如果你遵循图 2 的规则，拥有更新日志的服务器就不会被过时服务器的选举所打断，因此更有可能完成选举并成为领导者。
*   **遵循图 2 关于何时应开始选举的指示。** 特别要注意，如果你是一个候选者（即你正在运行选举），但选举定时器触发，你应该**开始另一次选举**。这对于避免因 RPC 延迟或丢失而导致系统停滞非常重要。
*   **在处理传入 RPC 之前，确保遵循“服务器规则”中的第二条规则。** 第二条规则规定：
    > 如果 RPC 请求或响应包含任期 T > currentTerm：设置 currentTerm = T，转换为跟随者 (§5.1)
    *   例如，如果你已经在当前任期投过票，而一个传入的 `RequestVote` RPC 的任期比你高，你应该**首先下台并采用他们的任期**（从而重置 `votedFor`），**然后**处理该 RPC，这将导致你**授予投票**！

**不正确的 RPC 处理程序**
尽管图 2 明确说明了每个 RPC 处理程序应该做什么，但一些微妙之处仍然容易被忽略。以下是我们反复看到的一些问题，你在实现时应留意：

*   如果某一步说“回复 false”，这意味着你应该**立即回复**，而**不执行任何后续步骤**。
*   如果你收到一个 `AppendEntries` RPC，其 `prevLogIndex` 指向你日志的**末尾之后**，你应该以与**你有该条目但任期不匹配**相同的方式处理它（即回复 false）。
*   即使领导者**没有发送任何条目**，也应执行 `AppendEntries` RPC 处理程序中的**检查 2**。
*   `AppendEntries` 最后一步（#5）中的 **`min` 是必需的**，并且需要根据**最后一个新条目的索引**来计算。仅仅让在 `lastApplied` 和 `commitIndex` 之间应用日志条目的函数在到达日志末尾时停止是**不够的**。这是因为在领导者发送给你的条目（这些条目都与你日志中的条目匹配）**之后**，你的日志中可能还有与领导者日志**不同**的条目。由于 #3 规定你只在有冲突条目时才截断日志，那些条目不会被移除，如果 `leaderCommit` 超出了领导者发送给你的条目，你可能会应用**错误的条目**。
*   **必须完全按照第 5.4 节所述实现“最新日志”检查。** 不要偷懒只检查日志长度！

**未能遵循规则**
虽然 Raft 论文非常明确地说明了如何实现每个 RPC 处理程序，但它也留下了一些规则和不变量的实现未指定。这些列在图 2 右侧的“**服务器规则**”块中。虽然其中一些相当不言自明，但也有一些需要你非常仔细地设计应用程序，以免违反规则：

*   如果在执行过程中任何时候 `commitIndex > lastApplied`，你应该应用特定的日志条目。你不一定要立即执行（例如，在 `AppendEntries` RPC 处理程序中），但**确保这个应用操作只由一个实体完成**很重要。具体来说，你需要要么有一个专用的“应用器”，要么在这些应用操作周围加锁，以免其他例程也检测到需要应用条目并尝试去应用。
*   确保你**定期**或在 `commitIndex` **更新后**（即 `matchIndex` 更新后）检查 `commitIndex > lastApplied`。例如，如果你在向对等节点发送 `AppendEntries` 的**同时**检查 `commitIndex`，你可能必须等到下一个条目被追加到日志后，才能应用你刚刚发送并得到确认的条目。
*   如果领导者发出一个 `AppendEntries` RPC，它被拒绝，但**不是**因为日志不一致（这只能发生在我们的任期已过时的情况下），那么你应该**立即下台**，并且**不要更新 `nextIndex`**。如果你更新了，而你又立即重新当选，可能会与 `nextIndex` 的重置产生竞争条件。
*   **领导者不允许**将 `commitIndex` 更新到**前一个任期**（或者未来的任期）的某个位置。因此，正如规则所说，你**特别需要**检查 `log[N].term == currentTerm`。这是因为如果条目不是来自其当前任期，Raft 领导者无法确定该条目是否真的已提交（并且将来永远不会改变）。这在论文的**图 8** 中有所说明。
*   一个常见的混淆来源是 `nextIndex` 和 `matchIndex` 的区别。特别是，你可能会观察到 `matchIndex = nextIndex - 1`，从而不实现 `matchIndex`。这是**不安全**的。虽然 `nextIndex` 和 `matchIndex` 通常同时更新为相似的值（具体来说，`nextIndex = matchIndex + 1`），但两者的目的**截然不同**。`nextIndex` 是对领导者与给定跟随者共享前缀的**猜测**。它通常相当**乐观**（我们共享所有内容），并且只在收到负面响应时向后移动。例如，当领导者刚当选时，`nextIndex` 被设置为日志末尾的索引。在某种程度上，`nextIndex` 用于**性能**——你只需要发送这些东西给这个对等节点。
    *   `matchIndex` 用于**安全**。它是对领导者与给定跟随者共享日志前缀的**保守**度量。`matchIndex` **绝不能**设置得过高的值，因为这可能导致 `commitIndex` 向前移动得太远。这就是为什么 `matchIndex` 初始化为 **-1**（即，我们不共享任何前缀），并且只在跟随者**正面确认**一个 `AppendEntries` RPC 时才更新。

**任期混淆**
任期混淆指的是服务器被来自**旧任期**的 RPC 搞糊涂了。总的来说，在**接收** RPC 时这不是问题，因为图 2 中的规则明确规定了当你看到旧任期时应该做什么。然而，图 2 通常没有讨论当你收到**旧的 RPC 回复**时应该做什么。根据经验，我们发现迄今为止**最简单**的做法是：首先记录回复中的任期（它可能比你当前的任期高），然后**将当前任期与你最初发送 RPC 时使用的任期进行比较**。如果两者不同，**丢弃回复并返回**。只有两者相同时，你才应继续处理该回复。在这里，通过一些巧妙的协议推理，你可能可以做进一步的优化，但这种方法似乎效果很好。**不这样做会导致一条漫长、曲折、充满血、汗、泪和绝望的道路。**

一个相关但不完全相同的问题是，**假设你的状态在发送 RPC 和接收回复之间没有改变**。一个很好的例子是，在收到 RPC 响应时设置 `matchIndex = nextIndex - 1` 或 `matchIndex = len(log)`。这是**不安全**的，因为这两个值自你发送 RPC 以来可能已经更新。相反，**正确**的做法是将 `matchIndex` 更新为你最初在 RPC 参数中发送的 `prevLogIndex + len(entries[])`。

**关于优化的补充说明**
Raft 论文包含几个值得关注的可选特性。在 6.824 中，我们要求学生实现其中两个：**日志压缩**（第 7 节）和**加速日志回溯**（第 8 页左上角）。前者对于避免日志无限增长是必要的，后者对于快速使过时的跟随者更新非常有用。

这些特性不是“核心 Raft”的一部分，因此在论文中没有像主共识协议那样受到那么多关注。日志压缩被覆盖得相当彻底（在图 13 中），但遗漏了一些如果你读得太随意可能会忽略的设计细节：

*   当对**应用程序状态**进行快照时，你需要确保应用程序状态对应于 Raft 日志中某个**已知索引**之后的状态。这意味着应用程序要么需要通知 Raft 快照对应的索引，要么 Raft 需要**延迟**应用额外的日志条目，直到快照完成。
*   文本没有讨论当服务器崩溃并**重新启动**时，现在涉及快照的恢复协议。特别是，如果 Raft 状态和快照是**分开提交**的，服务器可能在**持久化了一个快照**和**持久化更新的 Raft 状态**之间崩溃。这是一个问题，因为图 13 中的**步骤 7** 规定**必须丢弃快照覆盖的 Raft 日志**。
    *   如果服务器在恢复时，读取了**更新的快照**，但**过时的日志**，它最终可能会应用一些**已经包含在快照中**的日志条目。发生这种情况是因为 `commitIndex` 和 `lastApplied` **没有被持久化**，因此 Raft 不知道这些日志条目已经被应用。解决此问题的方法是向 Raft 引入一个**持久状态**，记录 Raft 持久化日志中的**第一个条目对应的“真实”索引**。然后可以将其与加载的快照的 `lastIncludedIndex` 进行比较，以确定要丢弃日志头部的哪些元素。

加速日志回溯优化的规范非常不明确，可能是因为作者认为它对于大多数部署来说不是必需的。从文本中不清楚跟随者返回的**冲突索引和任期**应如何被领导者用来确定使用什么 `nextIndex`。我们相信作者可能希望你遵循的协议是：

1.  如果跟随者的日志中**没有** `prevLogIndex`，它应该返回 `conflictIndex = len(log)` 和 `conflictTerm = None`。
2.  如果跟随者的日志中**有** `prevLogIndex`，但**任期不匹配**，它应该返回 `conflictTerm = log[prevLogIndex].Term`，然后**搜索**其日志中**第一个**条目任期等于 `conflictTerm` 的索引。
3.  收到冲突响应后，领导者应首先在其日志中**搜索** `conflictTerm`。如果找到具有该任期的条目，应将 `nextIndex` 设置为其日志中**该任期最后一个条目索引之后的一个**。
4.  如果找不到具有该任期的条目，它应设置 `nextIndex = conflictIndex`。

一个折中的解决方案是只使用 `conflictIndex`（而忽略 `conflictTerm`），这简化了实现，但领导者有时最终会向跟随者发送比**严格必要**更多的日志条目来使其更新。

**基于 Raft 的应用程序**
在 Raft 之上构建服务时（例如第二个 6.824 Raft 实验中的键/值存储），服务与 Raft 日志之间的交互可能很难正确处理。本节详细介绍了在构建应用程序时你可能会发现有用的一些开发过程方面。

**应用客户端操作**
你可能会对如何通过**复制日志**来实现应用程序感到困惑。你可能一开始让你的服务在收到客户端请求时，将该请求发送给领导者，等待 Raft 应用某些内容，执行客户端请求的操作，然后返回给客户端。虽然这在**单客户端**系统中没问题，但对于**并发客户端**则行不通。

相反，服务应构建为一个**状态机**，其中客户端操作使机器从一种状态转换到另一种状态。你应该在某个地方有一个**循环**，一次接受一个客户端操作（在**所有服务器上以相同的顺序**——这就是 Raft 的用武之地），并按顺序将每个操作应用到状态机上。**这个循环应该是你代码中唯一触及应用程序状态**（6.824 中的键/值映射）的部分。这意味着你面向客户端的 RPC 方法应该**简单地将客户端的操作提交给 Raft**，然后**等待**该操作被这个“**应用器循环**”应用。**只有当客户端的命令出现时**，才应执行它，并读取任何返回值。**注意，这包括读请求！**

这就引出了另一个问题：你**如何知道客户端操作何时完成**？在没有故障的情况下，这很简单——你只需等待你放入日志的东西回来（即，被传递给 `apply()`）。当这种情况发生时，你将结果返回给客户端。然而，如果**出现故障**会怎样？例如，客户端最初联系你时你是领导者，但此后**其他人当选了**，而你放入日志的客户端请求已被丢弃。显然你需要让客户端**重试**，但你怎么知道何时告诉他们错误？

解决这个问题的一个简单方法是，在插入客户端操作时，**记录该操作在 Raft 日志中出现的位置（索引）**。一旦该索引处的操作被发送到 `apply()`，你就可以根据**出现在该索引的操作是否确实是你放在那里的操作**来判断客户端的操作是否成功。**如果不是**，则发生了故障，可以向客户端返回错误。

**重复检测**
一旦你让客户端在遇到错误时**重试**操作，你就需要某种**重复检测机制**——如果一个客户端发送一个 APPEND 到你的服务器，没有收到回复，然后重新发送给下一个服务器，你的 `apply()` 函数需要确保该 APPEND **不会被执行两次**。为此，你需要为每个客户端请求提供某种**唯一标识符**，以便能够识别过去是否已经**看到**，更重要的是，**已经应用了**某个特定操作。此外，这个状态需要成为你**状态机的一部分**，以便你所有的 Raft 服务器**消除相同的重复**。

分配此类标识符的方法有很多。一种简单且相当高效的方法是给每个客户端一个**唯一标识符**，然后让他们用**单调递增的序列号**标记每个请求。如果客户端重新发送请求，它会**重用**相同的序列号。你的服务器跟踪它为每个客户端看到的**最新序列号**，并简单地**忽略**任何已经见过的操作。

**棘手的极端情况**
如果你的实现遵循上面给出的大纲，你很可能至少会遇到两个微妙的问题，如果不进行认真的调试，可能很难识别。为了节省你的时间，它们是：

1.  **重新出现的索引**：假设你的 Raft 库有某个 `Start()` 方法，它接受一个命令，并返回该命令被放置在日志中的**索引**（这样你就知道何时返回给客户端，如上所述）。你可能会假设永远不会看到 `Start()` **返回相同的索引两次**，或者至少，如果你看到相同的索引再次出现，第一次返回该索引的命令肯定失败了。事实证明，**即使没有服务器崩溃，这两者都不是真的**。
    *   考虑一个包含五个服务器 S1 到 S5 的场景。最初，S1 是领导者，其日志为空。
    *   两个客户端操作（C1 和 C2）到达 S1。
    *   `Start()` 为 C1 返回 1，为 C2 返回 2。
    *   S1 向 S2 发送包含 C1 和 C2 的 `AppendEntries`，但所有其他消息都丢失了。
    *   S3 作为候选者站出来。
    *   S1 和 S2 不会投票给 S3，但 S3、S4 和 S5 都会，所以 S3 成为领导者。
    *   另一个客户端请求 C3 到达 S3。
    *   S3 调用 `Start()`（返回 1）。
    *   S3 向 S1 发送 `AppendEntries`，S1 从其日志中**丢弃 C1 和 C2**，并添加 C3。
    *   S3 在向任何其他服务器发送 `AppendEntries` 之前发生故障。
    *   S1 站出来，因为其日志是最新的，它被选为领导者。
    *   另一个客户端请求 C4 到达 S1。
    *   S1 调用 `Start()`，返回 **2**（这也是 `Start(C2)` 返回的值）。
    *   S1 的所有 `AppendEntries` 都被丢弃，S2 站出来。
    *   S1 和 S3 不会投票给 S2，但 S2、S4 和 S5 都会，所以 S2 成为领导者。
    *   一个客户端请求 C5 到达 S2。
    *   S2 调用 `Start()`，返回 3。
    *   S2 成功向所有服务器发送 `AppendEntries`，S2 通过在下一个心跳中包含更新的 `leaderCommit = 3` 来向服务器报告。
    *   由于 S2 的日志是 `[C1, C2, C5]`，这意味着在索引 2 处提交（并在所有服务器，包括 S1 上应用）的条目是 **C2**。尽管在 S1 上最后一个返回索引 2 的客户端操作是 **C4**。

2.  **四路死锁**：发现这个问题的全部功劳归于另一位 6.824 助教 Steven Allen。他发现了这个在 Raft 之上构建应用程序时很容易陷入的**讨厌的四路死锁**。
    *   你的 Raft 代码，无论其结构如何，可能都有一个 `Start()` 类的函数，允许应用程序向 Raft 日志添加新命令。它也可能有一个循环，当 `commitIndex` 更新时，为 `lastApplied` 和 `commitIndex` 之间的每个日志元素调用应用程序的 `apply()`。这些例程可能都持有某个**锁 a**。在你的基于 Raft 的应用程序中，你可能在 RPC 处理程序中的某个地方调用 Raft 的 `Start()` 函数，并且在其他地方有一些代码，在 Raft 应用新日志条目时得到通知。由于这两者需要通信（即 RPC 方法需要知道它放入日志的操作何时完成），它们可能都持有某个**锁 b**。
    *   在 Go 中，这四段代码可能看起来像这样：
        ```go
        func (a *App) RPC(args interface{}, reply interface{}) {
            // ...
            a.mutex.Lock() // 获取应用锁 b
            i := a.raft.Start(args)
            // 更新某些数据结构，以便 apply 知道稍后通知我们
            a.mutex.Unlock() // 释放应用锁 b
            // 等待 apply 通知我们
            return
        }

        func (r *Raft) Start(cmd interface{}) int {
            r.mutex.Lock() // 获取 Raft 锁 a
            // 做一些事情来开始对这个新命令达成一致
            // 存储 cmd 被放置的日志中的索引
            r.mutex.Unlock() // 释放 Raft 锁 a
            return index
        }

        func (a *App) apply(index int, cmd interface{}) {
            a.mutex.Lock() // 获取应用锁 b
            switch cmd := cmd.(type) {
            case GetArgs:
                // 执行 get
                // 查看谁在监听这个索引
                // 用操作结果通知他们所有人
            // ...
            }
            a.mutex.Unlock() // 释放应用锁 b
        }

        func (r *Raft) AppendEntries(...) {
            // ...
            r.mutex.Lock() // 获取 Raft 锁 a
            // ...
            for r.lastApplied < r.commitIndex {
              r.lastApplied++
              r.app.apply(r.lastApplied, r.log[r.lastApplied]) // 调用 App.apply
            }
            // ...
            r.mutex.Unlock() // 释放 Raft 锁 a
        }
        ```
    *   现在考虑系统处于以下状态：
        *   `App.RPC` 刚刚获取了 `a.mutex` (应用锁 b) 并调用了 `Raft.Start`
        *   `Raft.Start` 正在等待 `r.mutex` (Raft 锁 a)
        *   `Raft.AppendEntries` 正持有 `r.mutex` (Raft 锁 a)，并刚刚调用了 `App.apply`
    *   现在我们有一个**死锁**，因为：
        *   `Raft.AppendEntries` 在 `App.apply` **返回**之前不会释放 `r.mutex` (Raft 锁 a)。
        *   `App.apply` 在获取 `a.mutex` (应用锁 b) **之前**无法返回。
        *   `a.mutex` (应用锁 b) 在 `App.RPC` **返回**之前不会被释放。
        *   `App.RPC` 在 `Raft.Start` **返回**之前不会返回。
        *   `Raft.Start` 在获取 `r.mutex` (Raft 锁 a) **之前**无法返回。
        *   `Raft.Start` 必须等待 `Raft.AppendEntries`。

你可以通过几种方法解决这个问题。**最简单**的一种是在 `App.RPC` 中**调用 `a.raft.Start` 之后**再获取 `a.mutex`。然而，这意味着对于 `App.RPC` 刚刚调用 `Raft.Start` 的操作，`App.apply` 可能在 `App.RPC` 有机会记录它希望被通知的事实**之前**就被调用。另一种可能产生更整洁设计的方案是让一个**专用的线程**在 Raft 中调用 `r.app.apply`。这个线程可以在每次 `commitIndex` 更新时得到通知，这样就不需要**持有锁**来应用，从而打破死锁。

-